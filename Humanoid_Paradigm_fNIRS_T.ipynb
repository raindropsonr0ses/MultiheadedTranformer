{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "jfjytIUB-egX",
        "F7axraOJDAsp",
        "7v_voxq-vRB0"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# fNIRS Data Preprocessing"
      ],
      "metadata": {
        "id": "FIV1SZq3tEDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from openpyxl import load_workbook\n",
        "\n",
        "class fNIRSDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom dataset for fNIRS HRF data from an Excel workbook.\n",
        "\n",
        "    Each sample corresponds to one subject–event combination (one column from the headers)\n",
        "    and returns (fNIRS_data, event_label).\n",
        "\n",
        "    fNIRS_data is a tensor of shape [2, num_channels, target_length]:\n",
        "      - 2: two signal types (0: HbO; 1: HbR)\n",
        "      - num_channels: expected 24\n",
        "      - target_length: number of time points after sliding-window processing\n",
        "\n",
        "    Event label mapping:\n",
        "      - If header has {\"S\", \"F\", \"H\"} (tertiary): {\"S\": 0, \"F\": 1, \"H\": 2}\n",
        "      - Otherwise (binary, only {\"F\",\"H\"}): {\"F\": 0, \"H\": 1}\n",
        "\n",
        "    Sliding-window processing:\n",
        "      Tertiary (has S):\n",
        "         - S: already 4549 points;\n",
        "         - F: valid length 4801 → windows [0:4549] and [252:252+4549] averaged → 4549 points;\n",
        "         - H: valid length 6801 → windows [0:4549] and [2252:2252+4549] averaged.\n",
        "      Binary (only F and H):\n",
        "         - F: valid length 4801 → take first 4801 points;\n",
        "         - H: valid length 6801 → windows [0:4801] and [2000:2000+4801] averaged.\n",
        "    \"\"\"\n",
        "    def __init__(self, excel_file, split=\"train\"):\n",
        "        self.excel_file = excel_file\n",
        "        wb = load_workbook(excel_file, read_only=True)\n",
        "        all_sheet_names = wb.sheetnames\n",
        "\n",
        "        # Group sheets by channel id using regex to extract \"number,number\"\n",
        "        channel_dict = {}\n",
        "        for sheet_name in all_sheet_names:\n",
        "            name_lower = sheet_name.lower()\n",
        "            measurement = None\n",
        "            if \"hbo\" in name_lower:\n",
        "                measurement = \"HbO\"\n",
        "            elif \"hbr\" in name_lower:\n",
        "                measurement = \"HbR\"\n",
        "            else:\n",
        "                continue\n",
        "            match = re.search(r'(\\d+,\\d+)', sheet_name)\n",
        "            if not match:\n",
        "                continue\n",
        "            channel_id = match.group(1)\n",
        "            channel_dict.setdefault(channel_id, {})[measurement] = sheet_name\n",
        "\n",
        "        valid_channels = {cid: sheets for cid, sheets in channel_dict.items()\n",
        "                          if \"HbO\" in sheets and \"HbR\" in sheets}\n",
        "        self.channels = sorted(valid_channels.keys())\n",
        "        self.num_channels = len(self.channels)\n",
        "        if self.num_channels == 0:\n",
        "            raise ValueError(\"No valid channels found. Ensure sheet names contain 'HbO' or 'HbR' and a channel id in the format 'number,number'.\")\n",
        "        self.hb_sheets = [(valid_channels[cid][\"HbO\"], valid_channels[cid][\"HbR\"]) for cid in self.channels]\n",
        "\n",
        "        # Extract header information from one HbO sheet.\n",
        "        sample_sheet = wb[self.hb_sheets[0][0]]\n",
        "        rows = list(sample_sheet.iter_rows(values_only=True))\n",
        "        header1 = list(rows[0])  # event labels\n",
        "        header2 = list(rows[1])  # subject identifiers (not used for classification)\n",
        "        expected_events = {\"S\", \"F\", \"H\"}\n",
        "        # If first cell not an expected event, assume timestamp column.\n",
        "        if header1[0] not in expected_events:\n",
        "            header1 = header1[1:]\n",
        "            header2 = header2[1:]\n",
        "            self.drop_first = True\n",
        "        else:\n",
        "            self.drop_first = False\n",
        "        self.sample_headers = list(zip(header1, header2))\n",
        "\n",
        "        # Split subjects into train (16 subjects) and test (4 subjects).\n",
        "        all_subjects = sorted(list(set([subj for (_, subj) in self.sample_headers])))\n",
        "        if len(all_subjects) != 20:\n",
        "            print(f\"Warning: Expected 20 subjects but found {len(all_subjects)}.\")\n",
        "        if split == \"train\":\n",
        "            selected_subjects = all_subjects[:16]\n",
        "        elif split == \"test\":\n",
        "            selected_subjects = all_subjects[16:]\n",
        "        elif split in [\"all\", \"validation\"]:\n",
        "            selected_subjects = all_subjects\n",
        "        else:\n",
        "            raise ValueError(\"Invalid split type. Must be 'train' or 'test'.\")\n",
        "        self.samples_meta = []\n",
        "        for col_idx, (event, subj) in enumerate(self.sample_headers):\n",
        "            if subj in selected_subjects:\n",
        "                self.samples_meta.append((event, subj, col_idx))\n",
        "        self.sampling_points = len(rows) - 2  # maximum available time points in sheet\n",
        "        wb.close()\n",
        "\n",
        "        # Set event mapping and target length based on header content.\n",
        "        if expected_events == {\"S\", \"F\", \"H\"}:\n",
        "            self.event_map = {\"S\": 0, \"F\": 1, \"H\": 2}\n",
        "            self.target_length = 4549\n",
        "            self.offset_F = 252      # 4801 - 4549\n",
        "            self.offset_H = 2252     # 6801 - 4549\n",
        "        else:  # binary: only F and H\n",
        "            self.event_map = {\"F\": 0, \"H\": 1}\n",
        "            self.target_length = 4801\n",
        "            self.offset_H = 2000     # 6801 - 4801\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples_meta)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        event, subj, col_idx = self.samples_meta[idx]\n",
        "        actual_col_idx = col_idx + 1 if self.drop_first else col_idx\n",
        "\n",
        "        wb = load_workbook(self.excel_file, read_only=True)\n",
        "        hbO_list = []\n",
        "        hbR_list = []\n",
        "        for hbO_sheet_name, hbR_sheet_name in self.hb_sheets:\n",
        "            # Extract HbO data.\n",
        "            sheet_hbO = wb[hbO_sheet_name]\n",
        "            col_vals_hbO = []\n",
        "            for row in sheet_hbO.iter_rows(min_row=3, values_only=True):\n",
        "                val = row[actual_col_idx] if actual_col_idx < len(row) else 0\n",
        "                if val is None:\n",
        "                    val = 0\n",
        "                col_vals_hbO.append(val * 1_000_000)\n",
        "            hbO_list.append(np.array(col_vals_hbO, dtype=np.float32))\n",
        "            # Extract HbR data.\n",
        "            sheet_hbR = wb[hbR_sheet_name]\n",
        "            col_vals_hbR = []\n",
        "            for row in sheet_hbR.iter_rows(min_row=3, values_only=True):\n",
        "                val = row[actual_col_idx] if actual_col_idx < len(row) else 0\n",
        "                if val is None:\n",
        "                    val = 0\n",
        "                col_vals_hbR.append(val * 1_000_000)\n",
        "            hbR_list.append(np.array(col_vals_hbR, dtype=np.float32))\n",
        "        wb.close()\n",
        "\n",
        "        HbO_data = np.stack(hbO_list, axis=0)  # [num_channels, T]\n",
        "        HbR_data = np.stack(hbR_list, axis=0)  # [num_channels, T]\n",
        "        fNIRS_data = np.stack([HbO_data, HbR_data], axis=0)  # [2, num_channels, T]\n",
        "        fNIRS_data = torch.tensor(fNIRS_data, dtype=torch.float32)\n",
        "\n",
        "        # Sliding-window processing based on whether header has \"S\" (tertiary) or not (binary).\n",
        "        if \"S\" in self.event_map:  # tertiary: target_length = 4549\n",
        "            if event == \"S\":\n",
        "                new_data = fNIRS_data[:, :, :self.target_length]\n",
        "            elif event == \"F\":\n",
        "                window1 = fNIRS_data[:, :, :self.target_length]\n",
        "                window2 = fNIRS_data[:, :, self.offset_F:self.offset_F+self.target_length]\n",
        "                new_data = (window1 + window2) / 2.0\n",
        "            elif event == \"H\":\n",
        "                window1 = fNIRS_data[:, :, :self.target_length]\n",
        "                window2 = fNIRS_data[:, :, self.offset_H:self.offset_H+self.target_length]\n",
        "                new_data = (window1 + window2) / 2.0\n",
        "            else:\n",
        "                new_data = fNIRS_data[:, :, :self.target_length]\n",
        "        else:  # binary: target_length = 4801\n",
        "            if event == \"F\":\n",
        "                new_data = fNIRS_data[:, :, :self.target_length]\n",
        "            elif event == \"H\":\n",
        "                window1 = fNIRS_data[:, :, :self.target_length]\n",
        "                window2 = fNIRS_data[:, :, self.offset_H:self.offset_H+self.target_length]\n",
        "                new_data = (window1 + window2) / 2.0\n",
        "            else:\n",
        "                new_data = fNIRS_data[:, :, :self.target_length]\n",
        "\n",
        "        event_label = self.event_map.get(event, -1)\n",
        "        return new_data, event_label\n",
        "\n",
        "excel_path = '/content/Subjectwise Conc (GLM+no MA) with S.xlsx'\n",
        "validation_excel_path = '/content/Subjectwise Validation Conc (GLM+no MA) with S.xlsx'\n",
        "train_dataset = fNIRSDataset(excel_path, split=\"train\")\n",
        "val_dataset   = fNIRSDataset(validation_excel_path, split=\"all\")\n",
        "test_dataset  = fNIRSDataset(excel_path, split=\"test\")\n",
        "\n",
        "print(\"Train samples:\", len(train_dataset))\n",
        "print(\"Validation samples:\", len(val_dataset))\n",
        "print(\"Test samples:\", len(test_dataset))\n",
        "\n",
        "sample_data, event_lbl = train_dataset[0]\n",
        "print(\"fNIRS data shape:\", sample_data.shape)\n",
        "print(\"Labels -> Event: {}\".format(event_lbl))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "O3r4wGED9UIB",
        "outputId": "6ef5955a-5898-43d7-953c-6774f6242e4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Expected 20 subjects but found 4.\n",
            "Train samples: 48\n",
            "Validation samples: 12\n",
            "Test samples: 12\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 5, got 2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-c674db5bc9e1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test samples:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0msample_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_lbl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_lbl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmusic_lbl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meyes_lbl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fNIRS data shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Labels -> Event: {}, Group: {}, Music: {}, Eyes: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_lbl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_lbl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmusic_lbl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meyes_lbl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_data, event_lbl = train_dataset[0]\n",
        "print(\"fNIRS data shape:\", sample_data.shape)\n",
        "print(\"Labels -> Event: {}\".format(event_lbl))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRjrd00oI4Cl",
        "outputId": "e08c5239-706b-4d97-dbb7-b4b11f501dbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fNIRS data shape: torch.Size([2, 24, 4549])\n",
            "Labels -> Event: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "train_loader = DataLoader(train_dataset, batch_size=3, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=3,shuffle=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=3,shuffle=False)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXZbMEMd9Vtl",
        "outputId": "58c3961d-3c4d-4a7b-ac89-a1aa22154366"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Shared Modules"
      ],
      "metadata": {
        "id": "QUvuhPka406q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from einops import rearrange, repeat\n",
        "from torch import einsum\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "# Shared Modules (Residual, PreNorm, FeedForward, Attention, Transformer)\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(x, **kwargs) + x\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        project_out = not (heads == 1 and dim_head == dim)\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        ) if project_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        b, n, _ = x.shape  # [B, seq_length, dim]\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n",
        "\n",
        "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
        "        mask_value = -torch.finfo(dots.dtype).max\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = F.pad(mask.flatten(1), (1, 0), value=True)\n",
        "            assert mask.shape[-1] == dots.shape[-1], 'mask has incorrect dimensions'\n",
        "            mask = rearrange(mask, 'b i -> b () i ()') * rearrange(mask, 'b j -> b () () j')\n",
        "            dots = dots.masked_fill(~mask, mask_value)\n",
        "\n",
        "        attn = dots.softmax(dim=-1)\n",
        "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        return self.to_out(out)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                Residual(PreNorm(dim, Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout))),\n",
        "                Residual(PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout)))\n",
        "            ]))\n",
        "    def forward(self, x, mask=None):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x, mask=mask)\n",
        "            x = ff(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "ha-PjxUD-jYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Shared Backbone (Version 1)"
      ],
      "metadata": {
        "id": "jfjytIUB-egX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Version 1: Shared Backbone with Separate Heads\n",
        "\n",
        "Concept\n",
        "A single fNIRS transformer backbone (modified version of your fNIRS_T) is used to extract a shared latent representation from the raw input.\n",
        "This latent representation is then passed into four separate classifier heads (each implemented using a transformer block followed by pooling and a linear classifier) for the different tasks (event, group, music, eye).\n",
        "\n",
        "Pros\n",
        "1. Efficiency: The backbone is computed only once for all tasks.\n",
        "2. Shared Features: If tasks are related, the shared features may improve performance through transfer learning.\n",
        "3. Parameter Savings: Fewer overall parameters than training four completely separate models.\n",
        "\n",
        "Cons\n",
        "1. Task Conflict: If the tasks require very different features, sharing may hurt performance."
      ],
      "metadata": {
        "id": "F7axraOJDAsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fNIRS Backbone: Modified fNIRS_T that returns latent features\n",
        "\n",
        "class fNIRS_T_Backbone(nn.Module):\n",
        "    \"\"\"\n",
        "    fNIRS-T backbone that outputs latent features (before final classification).\n",
        "    Input shape: [B, 2, fNIRS_channels, sampling_point]\n",
        "    \"\"\"\n",
        "    def __init__(self, sampling_point, dim, depth, heads, mlp_dim, pool='cls', dim_head=64, dropout=0., emb_dropout=0.):\n",
        "        super().__init__()\n",
        "        num_patches = 100\n",
        "        num_channels = 100\n",
        "\n",
        "        self.to_patch_embedding = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=2, out_channels=8, kernel_size=(5, 30), stride=(1, 4)),\n",
        "            Rearrange('b c h w -> b h (c w)'),\n",
        "            nn.Linear((math.floor((sampling_point-30)/4)+1)*8, dim),\n",
        "            nn.LayerNorm(dim)\n",
        "        )\n",
        "        self.to_channel_embedding = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=2, out_channels=8, kernel_size=(1, 30), stride=(1, 4)),\n",
        "            Rearrange('b c h w -> b h (c w)'),\n",
        "            nn.Linear((math.floor((sampling_point-30)/4)+1)*8, dim),\n",
        "            nn.LayerNorm(dim)\n",
        "        )\n",
        "        self.pos_embedding_patch = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
        "        self.cls_token_patch = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.dropout_patch = nn.Dropout(emb_dropout)\n",
        "        self.transformer_patch = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
        "        self.pos_embedding_channel = nn.Parameter(torch.randn(1, num_channels + 1, dim))\n",
        "        self.cls_token_channel = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.dropout_channel = nn.Dropout(emb_dropout)\n",
        "        self.transformer_channel = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
        "        self.pool = pool\n",
        "        self.to_latent = nn.Identity()\n",
        "\n",
        "    def forward(self, img, mask=None):\n",
        "        x = self.to_patch_embedding(img)\n",
        "        x2 = self.to_channel_embedding(img.squeeze())\n",
        "        b, n, _ = x.shape\n",
        "        cls_tokens_patch = repeat(self.cls_token_patch, '() n d -> b n d', b=b)\n",
        "        x = torch.cat((cls_tokens_patch, x), dim=1)\n",
        "        x += self.pos_embedding_patch[:, :(n + 1)]\n",
        "        x = self.dropout_patch(x)\n",
        "        x = self.transformer_patch(x, mask)\n",
        "\n",
        "        b, n, _ = x2.shape\n",
        "        cls_tokens_channel = repeat(self.cls_token_channel, '() n d -> b n d', b=b)\n",
        "        x2 = torch.cat((cls_tokens_channel, x2), dim=1)\n",
        "        x2 += self.pos_embedding_channel[:, :(n + 1)]\n",
        "        x2 = self.dropout_channel(x2)\n",
        "        x2 = self.transformer_channel(x2, mask)\n",
        "\n",
        "        # Pooling: choose either CLS token or mean pooling.\n",
        "        x = x.mean(dim=1) if self.pool == 'mean' else x[:, 0]\n",
        "        x2 = x2.mean(dim=1) if self.pool == 'mean' else x2[:, 0]\n",
        "\n",
        "        x = self.to_latent(x)\n",
        "        x2 = self.to_latent(x2)\n",
        "        latent = torch.cat((x, x2), 1)  # Shared latent space\n",
        "        return latent\n",
        "\n",
        "# TransformerClassifierHead (for each task)\n",
        "\n",
        "class TransformerClassifierHead(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes, depth=2, heads=8, dim_head=64, mlp_dim=64, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(*[\n",
        "            Residual(PreNorm(input_dim, Attention(input_dim, heads, dim_head, dropout)))\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(input_dim),\n",
        "            nn.Linear(input_dim, mlp_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(mlp_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # x - shape [B, seq_length, input_dim].\n",
        "        x = self.layers(x, mask=mask)\n",
        "        x = x.transpose(1, 2)\n",
        "        x = self.pool(x).squeeze(-1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# MultiHead fNIRS Classifier using a Shared Backbone\n",
        "\n",
        "class MultiHeadfNIRSClassifier_Shared(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-head classifier with a shared backbone and separate transformer classifier heads.\n",
        "    \"\"\"\n",
        "    def __init__(self, sampling_point, dim, depth, heads, mlp_dim, emb_dropout=0., dropout=0., pool='cls'):\n",
        "        super().__init__()\n",
        "        self.backbone = fNIRS_T_Backbone(sampling_point, dim, depth, heads, mlp_dim, pool, dim_head=64, dropout=dropout, emb_dropout=emb_dropout)\n",
        "        # The latent feature dimension is 2*dim due to concatenation.\n",
        "        input_dim_out = dim * 2\n",
        "        self.event_classifier = TransformerClassifierHead(input_dim_out, num_classes=5)\n",
        "        self.group_classifier = TransformerClassifierHead(input_dim_out, num_classes=2)\n",
        "        self.music_classifier = TransformerClassifierHead(input_dim_out, num_classes=2)\n",
        "        self.eye_classifier = TransformerClassifierHead(input_dim_out, num_classes=2)\n",
        "\n",
        "    def forward(self, img, mask=None):\n",
        "        # will obtain the shared latent representation from the backbone.\n",
        "        latent = self.backbone(img, mask)  # shape: [B, input_dim_out]\n",
        "        # a sequence dimension for the heads, add one (here we use a sequence length of 1).\n",
        "        latent_seq = latent.unsqueeze(1)  # shape: [B, 1, input_dim_out]\n",
        "        result1 = self.event_classifier(latent_seq, mask)\n",
        "        result2 = self.group_classifier(latent_seq, mask)\n",
        "        result3 = self.music_classifier(latent_seq, mask)\n",
        "        result4 = self.eye_classifier(latent_seq, mask)\n",
        "        return result1, result2, result3, result4"
      ],
      "metadata": {
        "id": "bkBYZxNV_bsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "lh13nOU6xzTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "num_epochs = 10\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model_shared = MultiHeadfNIRSClassifier_Shared(sampling_point, dim, depth, heads, mlp_dim, emb_dropout=emb_dropout, dropout=dropout, pool=pool)\n",
        "optimizer_shared = optim.Adam(model_shared.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model_shared.train()\n",
        "    for img, label_event, label_group, label_music, label_eye in train_loader:\n",
        "        optimizer_shared.zero_grad()\n",
        "        output_event, output_group, output_music, output_eye = model_shared(img)\n",
        "        loss_event = criterion(output_event, label_event)\n",
        "        loss_group = criterion(output_group, label_group)\n",
        "        loss_music = criterion(output_music, label_music)\n",
        "        loss_eye = criterion(output_eye, label_eye)\n",
        "        loss = loss_event + loss_group + loss_music + loss_eye\n",
        "        loss.backward()\n",
        "        optimizer_shared.step()\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "Xj1htgV7x2OJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Independent Model - Version 2"
      ],
      "metadata": {
        "id": "O2OsKF6p_smB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from einops import rearrange, repeat\n",
        "from torch import einsum\n",
        "from einops.layers.torch import Rearrange\n",
        "import torch.optim as optim\n",
        "\n",
        "# fNIRS_T Model (Original version with integrated classification head)\n",
        "\n",
        "class fNIRS_T(nn.Module):\n",
        "    \"\"\"\n",
        "    fNIRS-T model for classification.\n",
        "    Input shape: [B, 2, fNIRS_channels, sampling_point]\n",
        "    \"\"\"\n",
        "    def __init__(self, n_class, fNIRS_channels, sampling_point, dim, depth, heads, mlp_dim, pool='cls', dim_head=32, dropout=0., emb_dropout=0.):\n",
        "        super().__init__()\n",
        "        # Patch embedding branch with updated convolution parameters:\n",
        "        self.to_patch_embedding = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=2, out_channels=8, kernel_size=(6, 100), stride=(2, 30)), # 30/25 = 1.25 pattern 1, 2.5 pattern 2\n",
        "            Rearrange('b c h w -> b h (c w)'), # [B,2,24,4549] -> [B,8,10,149] -> [B,10,8*149] -> [B,10,1194]\n",
        "            nn.Linear((math.floor((sampling_point - 100) / 30) + 1) * 8, dim), # [1194,dim=128]\n",
        "            nn.LayerNorm(dim)\n",
        "        )\n",
        "        # Channel embedding branch remains similar to before:\n",
        "        self.to_channel_embedding = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=2, out_channels=8, kernel_size=(1, 30), stride=(1, 30)),\n",
        "            Rearrange('b c h w -> b h (c w)'),\n",
        "            nn.Linear((math.floor((sampling_point - 30) / 30) + 1) * 8, dim),\n",
        "            nn.LayerNorm(dim)\n",
        "        )\n",
        "        # Dynamically compute number of patches from fNIRS_channels:\n",
        "        num_patches = math.floor((fNIRS_channels - 6) / 2) + 1\n",
        "        self.pos_embedding_patch = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
        "        self.cls_token_patch = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.dropout_patch = nn.Dropout(emb_dropout)\n",
        "        self.transformer_patch = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
        "\n",
        "        # For channel embedding, the number of channels is fNIRS_channels:\n",
        "        num_channels = fNIRS_channels\n",
        "        self.pos_embedding_channel = nn.Parameter(torch.randn(1, num_channels + 1, dim))\n",
        "        self.cls_token_channel = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.dropout_channel = nn.Dropout(emb_dropout)\n",
        "        self.transformer_channel = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
        "\n",
        "        self.pool = pool\n",
        "        self.to_latent = nn.Identity()\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(dim * 2),\n",
        "            nn.Linear(dim * 2, n_class)\n",
        "        )\n",
        "\n",
        "    def forward(self, img, mask=None):\n",
        "        x = self.to_patch_embedding(img)\n",
        "        x2 = self.to_channel_embedding(img.squeeze())\n",
        "\n",
        "        b, n, _ = x.shape\n",
        "        cls_tokens_patch = repeat(self.cls_token_patch, '() n d -> b n d', b=b)\n",
        "        x = torch.cat((cls_tokens_patch, x), dim=1)\n",
        "        x += self.pos_embedding_patch[:, :(n + 1)]\n",
        "        x = self.dropout_patch(x)\n",
        "        x = self.transformer_patch(x, mask)\n",
        "\n",
        "        b, n, _ = x2.shape\n",
        "        cls_tokens_channel = repeat(self.cls_token_channel, '() n d -> b n d', b=b)\n",
        "        x2 = torch.cat((cls_tokens_channel, x2), dim=1)\n",
        "        x2 += self.pos_embedding_channel[:, :(n + 1)]\n",
        "        x2 = self.dropout_channel(x2)\n",
        "        x2 = self.transformer_channel(x2, mask)\n",
        "\n",
        "        x = x.mean(dim=1) if self.pool == 'mean' else x[:, 0]\n",
        "        x2 = x2.mean(dim=1) if self.pool == 'mean' else x2[:, 0]\n",
        "\n",
        "        x = self.to_latent(x)\n",
        "        x2 = self.to_latent(x2)\n",
        "        x3 = torch.cat((x, x2), 1)\n",
        "        return self.mlp_head(x3)\n"
      ],
      "metadata": {
        "id": "c4QjT3g9_xgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "vpa9nOxPEaRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Assume fNIRS_T and fNIRSClassifier are defined as in our previous code.\n",
        "class fNIRSClassifier(nn.Module):\n",
        "    def __init__(self, n_class, fNIRS_channels, sampling_point, dim, depth, heads, mlp_dim,\n",
        "                 dim_head=32, dropout=0., emb_dropout=0., pool='cls'):\n",
        "        super().__init__()\n",
        "        self.model = fNIRS_T(n_class=n_class,\n",
        "                             fNIRS_channels=fNIRS_channels,\n",
        "                             sampling_point=sampling_point,\n",
        "                             dim=dim,\n",
        "                             depth=depth,\n",
        "                             heads=heads,\n",
        "                             mlp_dim=mlp_dim,\n",
        "                             pool=pool,\n",
        "                             dim_head=dim_head,\n",
        "                             dropout=dropout,\n",
        "                             emb_dropout=emb_dropout)\n",
        "    def forward(self, x, mask=None):\n",
        "        return self.model(x, mask)\n",
        "\n",
        "# Hyperparameters\n",
        "num_epochs = 10\n",
        "learning_rate = 1e-6\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "sampling_point = 4549\n",
        "dim = 128\n",
        "depth = 3\n",
        "heads = 4\n",
        "dim_head = 32\n",
        "mlp_dim = 128\n",
        "dropout = 0.0\n",
        "emb_dropout = 0.0\n",
        "pool = 'cls'\n",
        "fNIRS_channels = 24\n",
        "n_class = 3  # or 2 based on your dataset\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Instantiate model and move to device.\n",
        "model = fNIRSClassifier(n_class=n_class,\n",
        "                        fNIRS_channels=fNIRS_channels,\n",
        "                        sampling_point=sampling_point,\n",
        "                        dim=dim,\n",
        "                        depth=depth,\n",
        "                        heads=heads,\n",
        "                        mlp_dim=mlp_dim,\n",
        "                        dim_head=dim_head,\n",
        "                        dropout=dropout,\n",
        "                        emb_dropout=emb_dropout,\n",
        "                        pool=pool)\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "checkpoint_dir = \"/content/event_checkpoints\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "# Training loop with validation evaluation.\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for img, label in train_loader:\n",
        "        img = img.to(device)\n",
        "        label = label.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(img)\n",
        "        loss = criterion(outputs, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Save checkpoint.\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, f\"model_epoch_{epoch+1}.pth\")\n",
        "    torch.save({\n",
        "        'epoch': epoch + 1,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': avg_loss,\n",
        "    }, checkpoint_path)\n",
        "    print(f\"Saved checkpoint: {checkpoint_path}\")\n",
        "\n",
        "    # Evaluate on validation set.\n",
        "    model.eval()\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "    with torch.no_grad():\n",
        "        for img, label in val_loader:\n",
        "            img = img.to(device)\n",
        "            label = label.to(device)\n",
        "            outputs = model(img)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_val += label.size(0)\n",
        "            correct_val += (predicted == label).sum().item()\n",
        "    val_accuracy = 100.0 * correct_val / total_val\n",
        "    print(f\"Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "# Final evaluation on test set.\n",
        "model.eval()\n",
        "correct_test = 0\n",
        "total_test = 0\n",
        "with torch.no_grad():\n",
        "    for img, label in test_loader:\n",
        "        img = img.to(device)\n",
        "        label = label.to(device)\n",
        "        outputs = model(img)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total_test += label.size(0)\n",
        "        correct_test += (predicted == label).sum().item()\n",
        "test_accuracy = 100.0 * correct_test / total_test\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "nbPbfaKtEdjt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "029ed5ca-79fb-4628-b672-876dd4b849f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Training Loss: 1.1521\n",
            "Saved checkpoint: /content/event_checkpoints/model_epoch_1.pth\n",
            "Validation Accuracy: 33.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validation"
      ],
      "metadata": {
        "id": "7v_voxq-vRB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in val_dataloader:\n",
        "    inputs, labels = batch\n",
        "    print(inputs[0][0][0][0].shape)\n",
        "    print(inputs[0][0][0][0])\n",
        "    print(inputs[0][0][0].shape) # the first index represents subject-event pair, 2nd index represents HbO or HbR\n",
        "    print(inputs[0][0][0])\n",
        "    print(inputs[0][0].shape)\n",
        "    print(inputs[0][0])\n",
        "    print(inputs[0].shape)\n",
        "    print(inputs[0])\n",
        "    print(inputs.shape)\n",
        "    print(inputs)\n",
        "    break\n",
        "\n",
        "# 2*24*6801*60\n",
        "# 19,586,880\n",
        "'''\n",
        "NOT USING ANYMORE\n",
        "Each event is of 270 sec duration (after padding event S and event F)\n",
        "F -> 270 (same padding technique as S), S->270 (179.92 secs original data other time segments consists of 0's) and H->270\n",
        "And sampling freq is 25Hz i.e data is captured once every 0.04 secs\n",
        "t1 (Onset), duration = 270, buffer_time=2 secs, [t1-2,t1+270)+1->data to extracted\n",
        "# sampling points = 272*25+1 = 6800 + 1 = 6801\n",
        "'''\n",
        "# 3 in position, black & white 1 in 2nd position\n",
        "# [32,3,64,64] -> looking at 32 images at once, 3 is the channels (Red, green or Blue)\n",
        "# [3,64,64] -> only image, 4096 pixels from all 3 colors or channels (single image)\n",
        "# [64,64] -> grid view, 4096 pixels of an image\n",
        "\n",
        "# [4,2,24,6801] -> we are looking at 4 subject-event pair data at once\n",
        "# [2,24,6801] -> we are looking at a subject-event pair data at once (HbO,HbR)\n",
        "# [24,6801] -> inside HbO or HbR (all 24 channels ki time series)\n",
        "# [6801] -> actual time series data of a single channel"
      ],
      "metadata": {
        "id": "UkO1Zkp9u27q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "99a66479-a747-490c-d5b6-db09820f8894"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples (subject-event combinations): 12\n",
            "Sampling points (from first channel sheet): 6801\n",
            "Number of channels (HbO/HbR pairs): 24\n",
            "fNIRS data shape: torch.Size([2, 24, 6801])\n",
            "Event label (mapped): 0\n",
            "torch.Size([6801])\n",
            "tensor([0.0458, 0.0564, 0.0669,  ..., 0.0000, 0.0000, 0.0000])\n",
            "torch.Size([24, 6801])\n",
            "tensor([[ 0.0458,  0.0564,  0.0669,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.6289, -0.6088, -0.5889,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.2758,  0.2937,  0.3117,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        ...,\n",
            "        [ 1.1112,  1.1138,  1.1169,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 1.0854,  1.1100,  1.1351,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 2.9804,  2.9705,  2.9626,  ...,  0.0000,  0.0000,  0.0000]])\n",
            "torch.Size([2, 24, 6801])\n",
            "tensor([[[ 0.0458,  0.0564,  0.0669,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [-0.6289, -0.6088, -0.5889,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.2758,  0.2937,  0.3117,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         ...,\n",
            "         [ 1.1112,  1.1138,  1.1169,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 1.0854,  1.1100,  1.1351,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 2.9804,  2.9705,  2.9626,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "        [[ 0.0878,  0.0833,  0.0783,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [-0.1409, -0.1617, -0.1827,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.2483,  0.2542,  0.2600,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         ...,\n",
            "         [ 0.2951,  0.3033,  0.3110,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.2967,  0.2930,  0.2890,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.9376,  0.9841,  1.0281,  ...,  0.0000,  0.0000,  0.0000]]])\n",
            "torch.Size([4, 2, 24, 6801])\n",
            "tensor([[[[ 0.0458,  0.0564,  0.0669,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [-0.6289, -0.6088, -0.5889,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.2758,  0.2937,  0.3117,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          ...,\n",
            "          [ 1.1112,  1.1138,  1.1169,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 1.0854,  1.1100,  1.1351,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 2.9804,  2.9705,  2.9626,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[ 0.0878,  0.0833,  0.0783,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [-0.1409, -0.1617, -0.1827,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.2483,  0.2542,  0.2600,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          ...,\n",
            "          [ 0.2951,  0.3033,  0.3110,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.2967,  0.2930,  0.2890,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.9376,  0.9841,  1.0281,  ...,  0.0000,  0.0000,  0.0000]]],\n",
            "\n",
            "\n",
            "        [[[ 0.6006,  0.6023,  0.6037,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 2.4712,  2.5026,  2.5325,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.6332,  0.6337,  0.6344,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          ...,\n",
            "          [ 0.5622,  0.5567,  0.5523,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.8118,  0.8099,  0.8081,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 4.2606,  4.2270,  4.1960,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[-0.2179, -0.2165, -0.2149,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [-0.8140, -0.8115, -0.8086,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.1674,  0.1649,  0.1625,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          ...,\n",
            "          [ 0.8580,  0.8542,  0.8511,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.3548,  0.3533,  0.3518,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 5.0645,  5.0214,  4.9833,  ...,  0.0000,  0.0000,  0.0000]]],\n",
            "\n",
            "\n",
            "        [[[-0.9101, -0.9069, -0.9032,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [-2.1214, -2.1184, -2.1147,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.1123,  0.1199,  0.1279,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          ...,\n",
            "          [ 0.6881,  0.6886,  0.6895,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0356,  0.0458,  0.0564,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.7968,  0.7851,  0.7747,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[-0.1015, -0.1047, -0.1079,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [-0.3373, -0.3404, -0.3434,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.2164,  0.2154,  0.2142,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          ...,\n",
            "          [ 0.5351,  0.5338,  0.5325,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.1246,  0.1238,  0.1228,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.5139,  0.4989,  0.4850,  ...,  0.0000,  0.0000,  0.0000]]],\n",
            "\n",
            "\n",
            "        [[[ 1.6172,  1.6647,  1.7134,  ...,  1.3647,  1.3762,  1.3877],\n",
            "          [ 1.3426,  1.3060,  1.2712,  ...,  0.5683,  0.5792,  0.5904],\n",
            "          [-0.2354, -0.2816, -0.3278,  ..., -0.7053, -0.6966, -0.6880],\n",
            "          ...,\n",
            "          [ 0.4471,  0.4173,  0.3868,  ...,  0.6416,  0.6448,  0.6480],\n",
            "          [ 0.0456,  0.0138, -0.0179,  ...,  1.0031,  1.0112,  1.0193],\n",
            "          [ 0.2094,  0.1700,  0.1309,  ...,  0.2971,  0.3071,  0.3172]],\n",
            "\n",
            "         [[-0.2045, -0.1974, -0.1896,  ...,  0.1149,  0.1173,  0.1197],\n",
            "          [-0.3990, -0.4342, -0.4685,  ...,  0.4406,  0.4429,  0.4449],\n",
            "          [-0.1284, -0.1503, -0.1723,  ..., -0.7880, -0.7880, -0.7881],\n",
            "          ...,\n",
            "          [ 0.1191,  0.1020,  0.0847,  ...,  0.2127,  0.2143,  0.2158],\n",
            "          [-0.3484, -0.3612, -0.3739,  ..., -0.0460, -0.0453, -0.0447],\n",
            "          [ 0.0247,  0.0058, -0.0129,  ..., -0.0713, -0.0663, -0.0613]]]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nEach event is of 270 sec duration (after padding event S and event F)\\nF -> 270 (same padding technique as S), S->270 (179.92 secs original data other time segments consists of 0's) and H->270\\nAnd sampling freq is 25Hz i.e data is captured once every 0.04 secs\\nt1 (Onset), duration = 270, buffer_time=2 secs, [t1-2,t1+270)+1->data to extracted \\n# sampling points = 270*25+1 = 6800 + 1 = 6801\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Validation Code\n",
        "# -----------------------------\n",
        "# Assuming you have a separate validation dataloader called validation_dataloader\n",
        "import os\n",
        "checkpoint_dir = \"/content/Checkpoints\"\n",
        "n_class = 3\n",
        "sampling_point = 6801\n",
        "dim = 64\n",
        "depth = 2\n",
        "heads = 8\n",
        "dim_head = 64\n",
        "mlp_dim = 64\n",
        "dropout = 0.0\n",
        "emb_dropout = 0.0\n",
        "pool = 'cls'\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# For example, load the checkpoint from the last epoch:\n",
        "checkpoint_to_load = os.path.join(checkpoint_dir, f\"model_epoch_{1}.pth\")\n",
        "model_val = fNIRSClassifier(n_class=n_class, sampling_point=sampling_point, dim=dim,\n",
        "                            depth=depth, heads=heads, mlp_dim=mlp_dim,\n",
        "                            dim_head=dim_head, dropout=dropout, emb_dropout=emb_dropout, pool=pool)\n",
        "model_val = model_val.to(device)\n",
        "model_val.load_state_dict(torch.load(checkpoint_to_load))\n",
        "model_val.eval()\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for img, label in validation_dataloader:\n",
        "        img = img.to(device)\n",
        "        label = label.to(device)\n",
        "        outputs = model_val(img)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        print(\"Predicted labels:\", predicted.cpu().tolist())\n",
        "        print(\"True labels:\", label.cpu().tolist())\n",
        "        total += label.size(0)\n",
        "        correct += (predicted == label).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"Validation Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "vzmGWsR3pCY2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4256f247-c0bc-41ae-88ec-70a98e400319"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted labels: [2, 2, 1, 2]\n",
            "True labels: [0, 1, 1, 1]\n",
            "Predicted labels: [2, 2, 2, 2]\n",
            "True labels: [0, 0, 2, 2]\n",
            "Predicted labels: [1, 2, 2, 2]\n",
            "True labels: [2, 1, 2, 0]\n",
            "Validation Accuracy: 33.33%\n"
          ]
        }
      ]
    }
  ]
}