# -*- coding: utf-8 -*-
"""fNIRS_T_Model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ejP7pEtUY-xkDV6fVeh3QVUnCWEWax9f
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from einops import rearrange, repeat
from torch import einsum
from einops.layers.torch import Rearrange
import torch.optim as optim

# fNIRS_T Model: Original version with integrated classification head

class fNIRS_T(nn.Module):
    """
    fNIRS-T model for classification.
    Input shape: [B, 2, fNIRS_channels, sampling_point]
    """
    def __init__(self, n_class, fNIRS_channels, sampling_point, dim, depth, heads, mlp_dim, pool='cls', dim_head=32, dropout=0., emb_dropout=0.):
        super().__init__()

        # Patch embedding branch with updated convolution parameters:
        self.to_patch_embedding = nn.Sequential(
            nn.Conv2d(in_channels=2, out_channels=8, kernel_size=(6, 100), stride=(2, 30)), # 30/25 = 1.25 pattern 1, 2.5 pattern 2
            Rearrange('b c h w -> b h (c w)'), # [B,2,24,4549] -> [B,8,10,149] -> [B,10,8*149] -> [B,10,1194]
            nn.Linear((math.floor((sampling_point - 100) / 30) + 1) * 8, dim), # [1194,dim=128]
            nn.LayerNorm(dim)
        )
        # Channel embedding branch remains similar to before:
        self.to_channel_embedding = nn.Sequential(
            nn.Conv2d(in_channels=2, out_channels=8, kernel_size=(1, 30), stride=(1, 30)),
            Rearrange('b c h w -> b h (c w)'),
            nn.Linear((math.floor((sampling_point - 30) / 30) + 1) * 8, dim),
            nn.LayerNorm(dim)
        )
        # Dynamically compute number of patches from fNIRS_channels:
        num_patches = math.floor((fNIRS_channels - 6) / 2) + 1
        self.pos_embedding_patch = nn.Parameter(torch.randn(1, num_patches + 1, dim))
        self.cls_token_patch = nn.Parameter(torch.randn(1, 1, dim))
        self.dropout_patch = nn.Dropout(emb_dropout)
        self.transformer_patch = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)

        # For channel embedding, the number of channels is fNIRS_channels:
        num_channels = fNIRS_channels
        self.pos_embedding_channel = nn.Parameter(torch.randn(1, num_channels + 1, dim))
        self.cls_token_channel = nn.Parameter(torch.randn(1, 1, dim))
        self.dropout_channel = nn.Dropout(emb_dropout)
        self.transformer_channel = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)

        self.pool = pool
        self.to_latent = nn.Identity()
        self.mlp_head = nn.Sequential(     # MLP head actually does the classification part
            nn.LayerNorm(dim * 2),         #Dimension(128 CLS tokens) is multiplied by 2 as each token from the two outputs: patch(spatial) and channel(temporal)
            nn.Linear(dim * 2, n_class)
        )

    def forward(self, img, mask=None):
        x = self.to_patch_embedding(img)
        x2 = self.to_channel_embedding(img.squeeze())

        b, n, _ = x.shape
        cls_tokens_patch = repeat(self.cls_token_patch, '() n d -> b n d', b=b)
        x = torch.cat((cls_tokens_patch, x), dim=1)
        x += self.pos_embedding_patch[:, :(n + 1)]
        x = self.dropout_patch(x)
        x = self.transformer_patch(x, mask)

        b, n, _ = x2.shape
        cls_tokens_channel = repeat(self.cls_token_channel, '() n d -> b n d', b=b)
        x2 = torch.cat((cls_tokens_channel, x2), dim=1)
        x2 += self.pos_embedding_channel[:, :(n + 1)]
        x2 = self.dropout_channel(x2)
        x2 = self.transformer_channel(x2, mask)

        x = x.mean(dim=1) if self.pool == 'mean' else x[:, 0]
        x2 = x2.mean(dim=1) if self.pool == 'mean' else x2[:, 0]

        x = self.to_latent(x)
        x2 = self.to_latent(x2)
        x3 = torch.cat((x, x2), 1)
        return self.mlp_head(x3)